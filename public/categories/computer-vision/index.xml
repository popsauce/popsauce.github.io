<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Popsauce&#39;s Blog</title>
    <link>http://localhost:1313/categories/computer-vision/</link>
    <description>Recent content in Computer Vision on Popsauce&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>karanlohaan0@gmail.com (Karan Lohaan)</managingEditor>
    <webMaster>karanlohaan0@gmail.com (Karan Lohaan)</webMaster><atom:link href="http://localhost:1313/categories/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Compute Efficient Object Detection</title>
      <link>http://localhost:1313/posts/computeefficientobjectdetection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/computeefficientobjectdetection/</guid>
      <description>&lt;p&gt;Resource Starved object detection&lt;/p&gt;
&lt;p&gt;I was looking for an object detection solution for keeping track of crop items and their timeline transition so one would have to identify and keep track of a crop&amp;rsquo;s state using a data set of
Object Detection has come a long way from relying on basic keypoint detection algorithms to cnns to vision transformers&lt;/p&gt;
&lt;p&gt;When it comes to performing object detection on a small microcontroller there are a lot of options but they dont work as well as one would like them to&lt;/p&gt;
&lt;h2 id=&#34;feature-based-object-detection&#34;&gt;Feature based object detection&lt;/h2&gt;
&lt;h2 id=&#34;cnn-based-object-detection-efficient-cnns&#34;&gt;CNN based object detection (Efficient CNN&amp;rsquo;s)&lt;/h2&gt;
&lt;h2 id=&#34;vision-transformer-based-object-detection&#34;&gt;vision transformer based object detection&lt;/h2&gt;
&lt;h2 id=&#34;explaining-different-benchmarks-for-compute-efficient-neural-networks&#34;&gt;Explaining different benchmarks for compute efficient neural networks&lt;/h2&gt;
&lt;h3 id=&#34;real-time-latency&#34;&gt;Real time Latency&lt;/h3&gt;
&lt;h3 id=&#34;accuracy&#34;&gt;Accuracy&lt;/h3&gt;
&lt;h3 id=&#34;precision&#34;&gt;Precision&lt;/h3&gt;
&lt;h2 id=&#34;factors-that-matter-for-small-form-factor-dev-boards&#34;&gt;Factors that matter for small form factor dev boards&lt;/h2&gt;
&lt;p&gt;Would it be possible to reduce the size of the packages and libraries that have to be installed&lt;/p&gt;
&lt;p&gt;Since the opencv cpp library has extensive documentation i think the smallest form factor would be to directly try to implement the libraries that are required so we would have to narrow down our&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;
&lt;h2 id=&#34;testing-different-recipes-on-a-dev-board&#34;&gt;Testing different recipes on a dev board&lt;/h2&gt;
&lt;p&gt;There are plenty of options to choose from when deciding on a combination of devboard and camera,&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Structures for 2D points</title>
      <link>http://localhost:1313/posts/spatialdatastructures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/spatialdatastructures/</guid>
      <description>&lt;h2 id=&#34;navigating-the-2d-plane&#34;&gt;Navigating the 2D Plane&lt;/h2&gt;
&lt;p&gt;2d plane requires certain data structures like quad tree bs a quick brown fox jumped over the pickled fence zebra with another drawn&lt;/p&gt;
&lt;h2 id=&#34;quad-tree&#34;&gt;Quad tree&lt;/h2&gt;
&lt;p&gt;KD&lt;/p&gt;
&lt;h2 id=&#34;bsp&#34;&gt;BSP&lt;/h2&gt;
&lt;h2 id=&#34;kd-tree&#34;&gt;KD tree&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>Meshes, Voxels, NURBS and T-Splines</title>
      <link>http://localhost:1313/posts/nurbstspline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/nurbstspline/</guid>
      <description>&lt;p&gt;While learning CAD basics I stumbled upon various CAD specific surface modelling primitives like NURBS and T Splines.
Ive tried to demystify some aspects of these surface representing functions here.&lt;/p&gt;
&lt;h2 id=&#34;newer-perspectives&#34;&gt;Newer Perspectives&lt;/h2&gt;
&lt;p&gt;The main motivation for going down this deep dive of understanding CAD surfaces was mainly because it is really different from other forms of representing 3d models that rely on using a set of discrete points, triangles, polygons.&lt;/p&gt;
&lt;p&gt;I also wanted to check for their viability for representing 3d models in a more compressed/continuous manner as storing a set of points/triangles/polygons as a set of functions could be beneficial if we&amp;rsquo;re talking about general regular geometry based objects.  u&lt;/p&gt;
&lt;h2 id=&#34;case-studies-for-different-shapes&#34;&gt;Case studies for different Shapes&lt;/h2&gt;
&lt;p&gt;cylinder
sphere
monkeyface
room model&lt;/p&gt;
&lt;h2 id=&#34;file-formats-for-exotic-shapes&#34;&gt;File Formats for Exotic Shapes&lt;/h2&gt;
&lt;p&gt;scale constrained models ?(do they fix the model geometry to a scale present in the real world or is it just an arbitrary set of points)&lt;/p&gt;
&lt;h3 id=&#34;bezier-curve&#34;&gt;Bezier Curve&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;non-uniform-rational-b-splines&#34;&gt;Non Uniform Rational B-Splines&lt;/h2&gt;
&lt;p&gt;the stonk&lt;/p&gt;
&lt;h2 id=&#34;t-splines&#34;&gt;T-Splines&lt;/h2&gt;
&lt;p&gt;T Splines are more recent compared to NURBS based surfaces. Some key differences are the fact that it includes&lt;/p&gt;
&lt;h2 id=&#34;tspline-viewer&#34;&gt;Tspline viewer&lt;/h2&gt;
&lt;h2 id=&#34;contents-of-a-cad-file&#34;&gt;Contents of a CAD file&lt;/h2&gt;
&lt;p&gt;step is kind of like the csv of the cad world as it is a platform agnostic file type&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;The NURBS Book by Les A. Piegl and Wayne Tiller: This book was published in 1995 a really great to the point resource
p&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Modelling Axolotl Embryos</title>
      <link>http://localhost:1313/posts/modellingaxolotls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/modellingaxolotls/</guid>
      <description>&lt;h2 id=&#34;project-details&#34;&gt;Project Details&lt;/h2&gt;
&lt;h2 id=&#34;1-project-abstract&#34;&gt;1. Project Abstract​:&lt;/h2&gt;
&lt;h3 id=&#34;11-what-is-the-project-about-&#34;&gt;1.1 What is the project about ?&lt;/h3&gt;
&lt;p&gt;The key objective of this project is to generate a 3d model based on a
set of 2d images of an axolotl embryo at a particular stage. The sample
dataset contains 2D images of an embryo captured using a microscope in a
flipping column.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image32.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 1 - Stage 4 Axolotl Embryos in different orientations&lt;/p&gt;
&lt;p&gt;The 2D image dataset of any embryo sample at a particular stage
typically contains 8 images per sample. With each image containing
different orientations of the embryo as shown in figure 1.&lt;/p&gt;
&lt;h3 id=&#34;12-why-is-this-project-important-&#34;&gt;1.2 Why is this project important ?&lt;/h3&gt;
&lt;p&gt;This project is meant as a research tool to aid researchers in modelling
axolotl embryos based on images generated using techniques mentioned in
the paper
&lt;a href=&#34;https://www.researchgate.net/publication/328615916_Acquisition_and_reconstruction_of_4D_surfaces_of_axolotl_embryos_with_the_flipping_stage_robotic_microscope&#34;&gt;&lt;!-- raw HTML omitted --&gt;[1]&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;
. Axolotl embryos are unique in their similarities to human embryos in
aspects like neural plate closure and tube formation in embryos and they
are often used as model organisms for studying embryogenesis due to the
transparent membrane of the embryo that allows us to analyse significant
changes in the organism across different stages.&lt;/p&gt;
&lt;p&gt;No open source alternatives exist for this type of a tool that can
reconstruct an ellipsoidal structure from a small set of 2D images using
uncalibrated reconstruction methods, this tool would help in recreating
more than just 3D models of axolotl embryos, as the method outlined here
can be applied to any model that resembles an ellipsoid or a sphere, for
example other embryos/cells, planets, asteroids, spherical objects, etc
can be potential subjects that can be modelled using the techniques
mentioned in this proposal.&lt;/p&gt;
&lt;h2 id=&#34;2-project-in-detail-and-planned-approach&#34;&gt;2. Project in Detail and Planned Approach:&lt;/h2&gt;
&lt;p&gt;Modelling the axolotl embryos from the given set of 2D images can be
divided into different sub problems that have to be solved step by step.&lt;/p&gt;
&lt;p&gt;So to get the final implementation of the minimum viable solution I have
divided my approach approach into 3 main categories : -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Extracting embryo images from given 2D images (2.2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Generating a 3D model based on given 2D images (2.3)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Projecting embryo images on the generated 3D model (2.4)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;21-overview-of-current-3d-reconstruction-methods&#34;&gt;2.1 Overview of current 3D reconstruction Methods:&lt;/h3&gt;
&lt;p&gt;Recreating an accurate 3D model is still an area of active research
where there are different domain specific problems that arise depending
on the method used, the type of data available and the complexity of the
model that has to be generated.&lt;/p&gt;
&lt;p&gt;So upon researching current reconstruction techniques I found that they
either rely heavily on additional data in the form of depth maps, camera
coordinates, etc to generate accurate models as outlined in this doc
&lt;a href=&#34;https://docs.google.com/document/d/13XlC-Vmw73AgCSLihCjD8btnN01Xk5J7LBVb5X2vs8Q/edit?usp=sharing&#34;&gt;&lt;!-- raw HTML omitted --&gt;[2]&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;
or require existing 3D model datasets for training a NN that generates a
probability density cloud after analysing images, since the microscope
image dataset has some limitations based on the number of images
available per sample and the lack of any other form of external data for
spatial information, we will have to create new custom methods to tackle
reconstruction that generate spatial information based on 2D images that
utilise properties of an ellipsoid to recreate a 3D model. A List of
research papers on 3D reconstruction techniques can be found here
&lt;a href=&#34;https://docs.google.com/document/d/1KiuWRnsuQq4DdXVvLSC7bKDpIxDu7hvBqVqMnQse3bA/edit?usp=sharing&#34;&gt;&lt;!-- raw HTML omitted --&gt;[3]&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;
for comparisons of different methods.&lt;/p&gt;
&lt;h3 id=&#34;22-extracting-embryo-images-from-2d-images&#34;&gt;2.2 Extracting embryo images from 2D images.&lt;/h3&gt;
&lt;h4 id=&#34;221-methods-to-extract-region-of-interest&#34;&gt;2.2.1 Methods to Extract Region of Interest&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Extracting embryo images accurately is very critical in generating a
model as we’ll be deriving all the necessary spatial information from
the features present in the extracted embryo image. Initially I was
planning on training a NN trained on a training dataset of self
cropped images to extract our Region of Interest (Embryo) from the
background (flipping column).&lt;/p&gt;
&lt;p&gt;But since our dataset has some unique aspects like a monochromatic
blue hue as the background we can just mask all blue hue pixels by
modifying HSV (hue, saturation, value) values based on a rgb histogram
of the pixel population (See figure 3).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image11.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image22.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image29.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 3 - Embryo images and their corresponding rgb pixel population
histogram, y axis represents pixel population, x axis represents hue
value of given pixel ranging from 0 to 255&lt;/p&gt;
&lt;h4 id=&#34;222-masking-hue-values-according-to-a-custom-threshold&#34;&gt;2.2.2 Masking hue values according to a custom threshold:&lt;/h4&gt;
&lt;p&gt;The usual opencv algorithms (for eg watershed algorithm) for segmenting
objects from background were adequate for most images but for some
embryo images like the ones which have both their light yellow side and
dark side in the same frame as shown in figure 4, the output was not
coming out accurately as most of these algorithms rely on selecting
shapes based on similar pixel value intensity, so they were treating the
lighter side and the darker side as 2 separate entities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image10.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 4 - Embryo with both light colored and dark colored side in same
image&lt;/p&gt;
&lt;p&gt;So instead of joining the entities in a separate step it was more
convenient to just remove the blue hue background as it had a more
consistent pixel value range of blue hues.&lt;/p&gt;
&lt;p&gt;So after identifying relevant hsv values of the background we can mask
the image in the following manner shown in figure 5.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image22.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image34.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image26.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 5 - Masking hues in the embryo image.&lt;/p&gt;
&lt;h4 id=&#34;223-additional-image-augmentation-methods-for-improving-contour-extraction&#34;&gt;2.2.3 Additional image augmentation methods for improving contour extraction:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For some embryo images (Figure 6) which have a yellowish-green hue in
the ventral side masking all relevant areas resulted in the embryo
losing some pixels shown in the figure. This would make it difficult
to outline the embryo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image11.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image39.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image16.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 6 - Masking hues in the embryo image.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So a workaround for this involves increasing the relative saturation
value (S) and decreasing the relative brightness value (L) to get a
more clear distinction between the embryo and the background that
results in a more clean outline of the embryo as shown in figure 8.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image27.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 7 - Diagram showing HSL spectrum&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image11.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image42.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 8 - Changing the S and L values to get a cleaner outline of ROI&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image42.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image13.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 9 - RGB pixel population histogram of the augmented image&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can then get a more accurate outline and crop out the image as
shown in figure 8.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;23-generating-a-3d-model-based-on-given-2d-images&#34;&gt;2.3 Generating a 3D model based on given 2D images&lt;/h3&gt;
&lt;p&gt;The 3D model that we are going to generate would resemble an irregular
ellipsoid. To generate an accurate approximation of the 3D model we have
to solve 3 sub problems : -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Getting accurate outlines/contours (2.3.1)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Finding the approximate axis of rotation (2.3.2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Finding the angle of rotation for each contour by measuring the
displacement of corresponding points wrt the axis (2.3.3)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Generating a 3D mesh based on the point cloud (2.3.4)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image36.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 10 - Overview of the main idea, the first diagram represents a
single contour, the second diagram represents the 3d figure i.e by
rotating the outline according to the angle of rotation we can
generate a very close approximation of the actual 3D model that gets
better with more images per sample.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;231-getting-accurate-outlinescontours&#34;&gt;2.3.1 Getting accurate outlines/contours:&lt;/h4&gt;
&lt;p&gt;The 3D model that we are going to generate would resemble an irregular
ellipsoid. The first step in generating the 3D model is extracting
outlines from all embryo images in the set.&lt;/p&gt;
&lt;h2 id=&#34;mediaimage24pngmediaimage41pngmediaimage18png&#34;&gt;&lt;img src=&#34;media/image24.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image41.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image18.png&#34; alt=&#34;&#34;&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;media/image3.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image15.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image12.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 11- Rough Outlines/Contours of 3 Axolotl embryos&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The contour plots will be used as 2D numpy arrays of x, y coordinates
for further transformation steps.&lt;/p&gt;
&lt;p&gt;Since most samples have 8 images we’ll get 8 outlines per sample. And
after applying image filters for smoothening the ROI we can get a
smoother outline of the contour&lt;/p&gt;
&lt;p&gt;As shown in figure 12&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image17.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 12- Smoothened Outline/Contour of Axolotl embryo&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image21.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image31.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image28.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image33.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image33.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image33.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image35.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image30.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 13- All Outlines/Contours of a stage 4 axolotl embryo in 8
different orientations&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After getting all embryo outlines we’ll align them on the y axis and
then modify the numpy array of 2D coordinates as outlined in section
2.3.4 to generate a 3D point cloud.&lt;/p&gt;
&lt;p&gt;Depending on the no. of images per set this method would be able to
generate better approximations that approach the ideal model which
gets better with more images per sample.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;232-finding-the-approximate-axis-of-rotation&#34;&gt;2.3.2 Finding the approximate axis of rotation:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;For finding the approximate axis of rotation we can either take the
mean of the extreme points in the numpy array of 2D coordinates of the
contour, and generate a vertical line between them. For example, find
the max and min of y coordinate values and take the mean of their
distance in the x axis to get the x coordinate of the corresponding
axis of rotation, or use a centroid based on the given coordinates in
the numpy array.&lt;/p&gt;
&lt;p&gt;Or we can rely on finding a vertical axis that remains the same size
across the set of images taken 2 at a time and is closest to the
centroid. So selecting the longest vertical line and tracing it across
images can also reveal accurate approximations for axis of rotation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;233-finding-the-angle-of-rotation-for-each-contour&#34;&gt;2.3.3 Finding the angle of rotation for each contour:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;We then need to find the angle of rotation for each image,there are
multiple ways to find out the angle at which we’ll place our outlines
like, by checking the displacement of similar points with respect to
an approximate axis of rotation.&lt;/p&gt;
&lt;p&gt;So we will go through the sample set of 8 images and evaluate the
angle of rotation between each pair after going through them in their
normal order of rotation which will give us 7 values for angle of
rotation of the embryo in adjacent frames.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image45.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 14- Approximating the angle of rotation based on a set of 8
corresponding points in the two adjacent images&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image25.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 15 - Another example showing the method of finding angle of
rotation by checking for displacement of corresponding points.&lt;/p&gt;
&lt;h4 id=&#34;234-generating-a-3d-mesh-based-on-the-generated-point-cloud&#34;&gt;2.3.4 Generating a 3D mesh based on the generated point cloud:&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Now since we have both the contours and their angle of rotation we
apply the 3D transformation formula for rotating our contours about
the y axis and generate an array of x,y,z coordinates from the numpy
array of x, y coordinates of the contour.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 16- Transformation of coordinates after rotation about z axis
where A= x1, y1, z1 and A’= x2, y2, z1&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So a point on the contour say P( x, y) would be P’( x*sin(∅), y,
x*cos(∅) ) on the 3D ellipsoid upon rotation about the y axis. Where
∅ is the angle of rotation.&lt;/p&gt;
&lt;p&gt;So after getting the angle of rotation we can then align all outlines
to the XY Plane by aligning their axis of rotation to the y axis and
then apply the following transformation formula to get the new x y z
coordinates after rotating the outline into the plane about the axis
of rotation by an angle ∅.&lt;/p&gt;
&lt;p&gt;From P to P’ we get the following transformation (x, y, 0) to ( x *
cos(∅), y , x * sin(∅) ) as explained in figure 17 where the x
coordinate becomes x*cos(∅), y coordinate remains the same as we’re
rotating the figure about y axis and z coordinate is x*sin(∅) as
shown in the XZ plane diagram in figure 17.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image44.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 17- Diagram explaining the transformation formula for getting 3D
coordinates by rotating 2D contour about Y axis&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So after converting our 2d numpy array of coordinates to a 3d array.&lt;/p&gt;
&lt;p&gt;We can then generate a 3D scatter plot and generate a point cloud of
the embryo surface.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 18 - A regular ellipsoidal point cloud&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Then for generating the 3D mesh we can use Open3D for generating the
mesh that generates surfaces based on principles similar to nearest
neighbour algorithms for generating the mesh based on point
coordinates.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image40.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 19 - Mesh generated from the ellipsoidal point cloud&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can then export the 3D model in any specified format using python
stl and Open3D libraries.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;24-projecting-embryo-images-on-the-generated-3d-model&#34;&gt;2.4 Projecting embryo images on the generated 3D model&lt;/h3&gt;
&lt;p&gt;There are various projection methods that outline how to go about
projecting an image to a 3D model in the Open3D, Scikit and PyOpenGL
libraries. Image projection requires transforming the perspective image
to a projection texture that can be applied to the model.&lt;/p&gt;
&lt;p&gt;Out of the existing projection methods cylindrical projection methods
would suit this model the best as we are dealing with an irregular
ellipsoid. We can generate an approximate equirectangular image and then
apply it to our generated model. An example can be seen in figure&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;media/image20.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image23.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Figure 20 - Projection image of jupiter, projection image wrapped around
a model.&lt;/p&gt;
&lt;h4 id=&#34;241-use-image-stitching-algorithms-to-stitch-multiple-projection-images&#34;&gt;2.4.1 Use image stitching algorithms to stitch multiple projection images:&lt;/h4&gt;
&lt;p&gt;We’ll be using the extracted embryo images from section 2.2 to generate
the projection image.&lt;/p&gt;
&lt;p&gt;Where we’ll stitch multiple projection images into a single image using
opencv’s image stitching algorithms.The final image made up of multiple
stitched images would then be used as a texture or a projection image
that will be wrapped around the 3D model.&lt;/p&gt;
&lt;h4 id=&#34;242-projecting-the-final-projection-image-to-the-generated-3d-model&#34;&gt;2.4.2 Projecting the final projection image to the generated 3D model:&lt;/h4&gt;
&lt;p&gt;Similarly we can try mapping our extracted embryo to a sphere as shown
in figure 21.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image19.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image9.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;media/image38.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 21- Projection image of the embryo wrapped around a sphere&lt;/p&gt;
&lt;p&gt;But the main objective is mapping it onto our 3D generated model&lt;/p&gt;
&lt;p&gt;We could either rely on the image projection method or generate a depth
map based on our model.&lt;/p&gt;
&lt;p&gt;An example of an approximate mockup of the 3D model using 2 sample
images is shown in figure 22&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;media/image37.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure 22 - Projection image of the embryo wrapped around an early
mockup of the generated model.&lt;/p&gt;
&lt;h3 id=&#34;25-deploying-the-tool&#34;&gt;2.5 Deploying the tool:&lt;/h3&gt;
&lt;h4 id=&#34;251-publishing-it-to-pypi&#34;&gt;2.5.1 Publishing it to PyPI:&lt;/h4&gt;
&lt;p&gt;Since im using python libraries like OpenCV, Open3D, PyOpenGL,
matplotlib, basemap, i can deploy my solution by creating a python
package and publish it to PyPI (Python Package Index) for making it
accessible to all users as an installable library/package. This
implementation is the easiest as it doesn’t require any changes to the
main project code as it can be used as is in any python environment.&lt;/p&gt;
&lt;h4 id=&#34;252-deploying-the-script-as-a-web-application&#34;&gt;2.5.2 Deploying the script as a web application:&lt;/h4&gt;
&lt;p&gt;The other option is deploying my solution as a web application on a site
like heroku. I&amp;rsquo;ll set up my python scripts using flask and heroku CLI,
this method would have to comply with the site related restrictions and
would require tweaking libraries or other dependencies to reduce
complete project size. The heroku deployment option is very
straightforward for hosting python scripts and there is extensive
documentation for the same.&lt;/p&gt;
&lt;h4 id=&#34;253-adding-documentation&#34;&gt;2.5.3 Adding Documentation:&lt;/h4&gt;
&lt;p&gt;I’ll add extensive documentation using github pages / read the docs / mk
docs along with the implementation steps to help in ease of access as
well as improving readability.&lt;/p&gt;
&lt;h2 id=&#34;31-minimum-set-of-deliverables&#34;&gt;3.1 Minimum set of deliverables:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Extracting embryo images from given 2D images (2.2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Generating a 3D model based on given 2D images (2.3)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Projecting embryo images on the generated 3D model (2.4)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Deploying the python script (2.5)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;32-additional-features-that-can-be-added-if-extra-time-is-there&#34;&gt;3.2 Additional features that can be added if extra time is there:&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Additional features that can be added:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Displaying details like number, projected area, approximate
volume, hsv heatmap of individual cells in each embryo stage to
check for abnormalities and defects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Outlining and labelling each segmented section in the generated
model for easier classification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-tentative-timeline&#34;&gt;4. Tentative timeline:&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;5-plan-for-communication-with-mentors&#34;&gt;5. Plan for communication with mentors:&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll continue communicating with my mentors and keep them updated on my
progress during the devoworm weekly meetings like I have been doing in
the past. I will be active on all the mediums I have provided in my
contact details. I’ll also keep on updating minor progress milestones on
slack.&lt;/p&gt;
&lt;h2 id=&#34;6-candidate-details&#34;&gt;6. Candidate Details:&lt;/h2&gt;
&lt;h3 id=&#34;61-why-do-i-want-to-do-this-project-&#34;&gt;6.1 Why do I want to do this project ?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I’ve previously worked on computer vision projects related to image
classification and feature extraction and was always interested in
delving more into the spatial learning aspect side of things as these
techniques are being implemented in most real life applications like
self driving cars to area mapping drones, but felt they were too
difficult to try out, but I’ve been learning more methods in the
domain of 3D reconstruction through the ideas tested in this project
which has definitely improved my understanding about the process.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also the added benefit of working with a real dataset that can
be used as a research tool which is more exciting to work on compared
to a hobby project and has a lot of scope for usage in other areas as
irregular ellipsoid 3D reconstruction can be done for a much broader
set of objects, which really pushed me to delve deeper into exploring
ideas relevant for this project.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;62-relevant-project-experience-and-past-contributions&#34;&gt;6.2 Relevant Project Experience and Past contributions&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I have experience in designing CAD models based on complex shapes that
are used as components for Hardware Projects using a 3D printer. I
have also created a python script for generating custom 3D templates
for imprinting text and logos for a given 3d model using a general cli
interface by running the python script as described in
&lt;a href=&#34;https://github.com/popsauce/3D-imprint-generator&#34;&gt;&lt;!-- raw HTML omitted --&gt;[4]&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;I have been actively working on this project for the past few weeks
and have been updating my progress using presentations and mockups in
the weekly meetings of my organisation
&lt;a href=&#34;https://www.youtube.com/channel/UChGTq41_rJwmZ1I4j7SezWQ&#34;&gt;&lt;!-- raw HTML omitted --&gt;[5]&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;63-is-this-the-only-project-im-applying-for&#34;&gt;6.3 Is this the only project I’m applying for?&lt;/h3&gt;
&lt;p&gt;Yes, this is the only project that I am applying for.&lt;/p&gt;
&lt;h3 id=&#34;64-working-time-and-commitments-&#34;&gt;6.4 Working time and commitments :&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I plan to work full time on this project. I’ll be dedicating most of
my time after classes to get things done as per the timeline, if not
ahead of it. Giving me 40 to 50 hours every week including weekends.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;65-other-plans-for-summer-&#34;&gt;6.5 Other plans for summer :&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I don&amp;rsquo;t have any major commitments during the project period that will
hinder my progress on the project. I’ll just be having my end semester
exams in my college that will get over by May end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;appendix-&#34;&gt;Appendix :&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Object detection on the edge on a budget</title>
      <link>http://localhost:1313/posts/budgetfriendlycamera/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/budgetfriendlycamera/</guid>
      <description>&lt;p&gt;I was on the lookout for a custom object detection solution for 2 projects that i had in mind. Mostly to keep track of certain items and do some estimation based tasks like inventory management and update their status in real time.&lt;/p&gt;
&lt;p&gt;For example a scenario where a desk is in the field of view of the camera it should be able to detect objects present on the desk and then update a table of of values that maintain things present on the desk.&lt;/p&gt;
&lt;p&gt;So starting with some basic design considerations we need the following things more or less :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Portable camera for capturing a video stream for object detection&lt;/li&gt;
&lt;li&gt;Real time camera streaming (good to have)&lt;/li&gt;
&lt;li&gt;Adequate compute for object detection&lt;/li&gt;
&lt;li&gt;A way to interface with camera and schedule cron jobs if possible&lt;/li&gt;
&lt;li&gt;Extracting information from the video scene and storing it in a csv file containing the updated values of the last streaming session.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;potential-options-for-a-portable-camera&#34;&gt;Potential options for a portable camera&lt;/h2&gt;
&lt;p&gt;Camera module is probably the most important thing in this project so here are the potential options when it comes to digital cameras -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;camera based SoC / microcontroller&lt;/li&gt;
&lt;li&gt;webcameras&lt;/li&gt;
&lt;li&gt;consumer grade portable digital cameras like gopros&lt;/li&gt;
&lt;li&gt;smartphone cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What varies wildly amongst all these offerings is their cost, the size of their image sensors, thier recording resolution ranges and all these other variables&lt;/p&gt;
&lt;p&gt;I wont mention all the options that i went through but some of the major ones that stood out for my use case were esp32 based cameras, stm32f4 /f7 boards with openvision camera modules, raspberry pi soc with camera modules.&lt;/p&gt;
&lt;p&gt;And i even considered webcameras and old smartphone cameras cause i think its much better to go for an old smartphone camera as its the best quality camera you can get on a budget but still i wanted to test an soc based solution so i mostly went with using an old smartphone camera for testing the setup and a raspberry pi 0 2 w board for my image processing needs.&lt;/p&gt;
&lt;p&gt;Some options that i was considering at the time are listed here :-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Esp 32 with openvision camera modules like ov2640, 5640, 7670, 9655.&lt;/li&gt;
&lt;li&gt;Raspberry Pi modules ranging from Raspberry Pi Zero 2 W to Raspberry Pi 4 with their respective camera modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The sections below detail how to setup both types of cameras&lt;/p&gt;
&lt;h2 id=&#34;potential-options-for-compute&#34;&gt;Potential Options for Compute&lt;/h2&gt;
&lt;p&gt;Now this part is interesting cause this aspect was really hard to pin down and a lot of my inhibitions were really put to test. First I ruled out all the recent SOTA models as running a ViT based object detection solution would be a bit too overkill and expensive no matter how badly i wanted to go down this route. So after going through a lot of potential options these were what i nailed it down to :&lt;/p&gt;
&lt;p&gt;Try and see if I can make it work with a feature detection based algorithm if it could satisfy my use case as itll be easiest to setup and really efficient&lt;/p&gt;
&lt;p&gt;But before training anything fancy I trained a ResNet on my laptop to see how it fares as it is the easiest to implement&lt;/p&gt;
&lt;h2 id=&#34;setting-up-the-rpi-0-2-w&#34;&gt;Setting up the RPi 0 2 W&lt;/h2&gt;
&lt;p&gt;So as a potential portable battery powered camera this was a good choice compared to a smartphone as it costs $5 for microsd card, $15 for the rpi 0 2 w, $15 for the v2.1 camera module and $15-30 for a powerbank (10 Ah to 20 Ah) so around $50.
An old smartphone camera might cost around this much if not more like an old pixel camera or and old iphone.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;For this setup im using the 64bit Bullseye lite os based on debian 11, while flashing the image it is necessary to use the raspi-imager provided by rpi especialy for this headless setup as that is the only way to enable and run ssh.&lt;/p&gt;
&lt;p&gt;Earlier i used to use startup disk creator with an empty ssh file into the boot drive which enabled a default user and password but that has been disabled now.&lt;/p&gt;
&lt;p&gt;In the raspi imager ui there will be additional options for entering network details and enabling ssh which is pretty straightforward. It is neccessary to enter the network details for this headless setup as we&amp;rsquo;ll be connecting to our rpi using ssh over a wifi network so this has to be set during the image flashing process itself.&lt;/p&gt;
&lt;p&gt;After flashing the os to the micro sd card we can put it into our rpi and boot it up.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-rpi-as-a-headless-camera&#34;&gt;Setting up rpi as a headless camera&lt;/h2&gt;
&lt;p&gt;For our headless setup we ssh into our rpi and set it up without a screen so for that we&amp;rsquo;ll have to use the ssh username and password that we had set while flashing the image. Along with this we connect the v2 camera module to the mipi connector.&lt;/p&gt;
&lt;p&gt;After booting the rpi we need to set the camera settings by selecting the interface options using &lt;code&gt;sudo raspi-config&lt;/code&gt; command&lt;/p&gt;
&lt;p&gt;After booting the rpi we can confirm whether the camera is working or not by using the libcamera library,
libcamera-hello
libcamera-vid&lt;/p&gt;
&lt;h2 id=&#34;tradeoffs-between-live-streaming-video-vs-storing-and-transferring-video-files&#34;&gt;Tradeoffs between live-streaming video vs storing and transferring video files&lt;/h2&gt;
&lt;h2 id=&#34;media-server&#34;&gt;Media Server&lt;/h2&gt;
&lt;p&gt;For realtime streaming its essential to choose a lightweight media library that will convert the raw footage to rtsp if the stream&lt;/p&gt;
&lt;p&gt;Would it be possible to reduce the size of the packages and libraries that have to be installed&lt;/p&gt;
&lt;p&gt;Since the opencv cpp library has extensive documentation i think the smallest form factor would be to directly try to implement the libraries that are required so we would have to narrow down our&lt;/p&gt;
&lt;h2 id=&#34;feature-detection&#34;&gt;Feature Detection&lt;/h2&gt;
&lt;h2 id=&#34;testing-different-recipes-on-a-dev-board&#34;&gt;Testing different recipes on a dev board&lt;/h2&gt;
&lt;p&gt;There are plenty of options to choose from when deciding on a combination of devboard and camera,&lt;/p&gt;
&lt;p&gt;questions regarding pretraining transformer for video data&lt;/p&gt;
&lt;p&gt;labelled data for objects&lt;/p&gt;
&lt;p&gt;depth estimation for arms and arm positions ?&lt;/p&gt;
&lt;p&gt;The question&lt;/p&gt;
&lt;p&gt;distribution of labous instead of having a transformer do it all&lt;/p&gt;
&lt;p&gt;object detection :vit transformer or keypoint detection ? or yolo a&lt;/p&gt;
&lt;p&gt;object position :depth estimation&lt;/p&gt;
&lt;p&gt;set of actions that can be done on the object llm&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>placeholder</title>
      <link>http://localhost:1313/posts/placeholder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/placeholder/</guid>
      <description>&lt;p&gt;So&lt;/p&gt;
&lt;h2 id=&#34;understanding-advanced_xorpy&#34;&gt;Understanding advanced_xor.py&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>Scaling Image Processing</title>
      <link>http://localhost:1313/posts/scalingimageprocessing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>karanlohaan0@gmail.com (Karan Lohaan)</author>
      <guid>http://localhost:1313/posts/scalingimageprocessing/</guid>
      <description>&lt;p&gt;Tools for all your image processing needs.&lt;/p&gt;
&lt;h2 id=&#34;image-processing-primer&#34;&gt;Image Processing Primer&lt;/h2&gt;
&lt;p&gt;Modify&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hue sat values&lt;/li&gt;
&lt;li&gt;scaling&lt;/li&gt;
&lt;li&gt;edge detection&lt;/li&gt;
&lt;li&gt;optical flow&lt;/li&gt;
&lt;li&gt;canny edges&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ocr
background removal&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;
&lt;h2 id=&#34;opencv&#34;&gt;OpenCV&lt;/h2&gt;
&lt;h2 id=&#34;imagemagick&#34;&gt;Imagemagick&lt;/h2&gt;
&lt;h2 id=&#34;ffmpeg&#34;&gt;FFMPEG&lt;/h2&gt;
&lt;h2 id=&#34;gnu-image-manipulation-program&#34;&gt;GNU Image Manipulation Program&lt;/h2&gt;
&lt;p&gt;so while so far we have just had command line tools or libraries why have i mentioned a gui based application. Its not like we&amp;rsquo;ll be able to open and edit each image on the gui right ?&lt;/p&gt;
&lt;p&gt;I get all the concerns but i think this tool is the most underrated tool when it comes to quickly prototyping different techniques to see which achieves the best results and then applying custom results in a way that no other tool can, let me explain why&lt;/p&gt;
&lt;p&gt;all the above tools are a great fit when you know exactly what you want to do to the image but for cases where its still unknown how to approach refining an image for a particular task i always always open GIMP, ive really grown to like it over the years.&lt;/p&gt;
&lt;p&gt;But the best feature of them all is the fact that you can write scripts to do the same ie apply the same setting to all the images that you want to apply it to.
There are primarily 2 ways to get this done&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Script Fu&lt;/li&gt;
&lt;li&gt;Python Fu&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;script-fu&#34;&gt;Script Fu&lt;/h3&gt;
&lt;h3 id=&#34;python-fu&#34;&gt;Python Fu&lt;/h3&gt;
&lt;h2 id=&#34;specific-augmentation-related-techniques-for-certain-applications&#34;&gt;Specific augmentation related techniques for certain applications&lt;/h2&gt;</description>
    </item>
    
  </channel>
</rss>
